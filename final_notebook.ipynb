{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](dogecoin.jpg \"Dogecoin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dogecoin](images/dogecoin.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hedgefund is seeking to profit from the volatility and inefficiences of the cryptocurrency market. They would like to focus on Dogecoin, a cryptocurrency with a $30B market cap. Their marketing team desires to better understand the sentiment around dogecoin, for understanding other owners and what words or phrases most reseonate with them. \n",
    "\n",
    "Dogecoin began as satire, using a meme of a Shiba Inu dog misspelled as \"doge.\" The makers, Billy Marcus and Jackson Palmer, created it in late 2013. Then launched a large campaing including sending the Jamaican Bobled team to the '14 olympics and sponsoring a Nascar. \n",
    "\n",
    "As a cryptocurrency, Dogecoin also relies on block-chain cryptography. All coin holder's carry a ledger. And miner's have to solve mathproblems to create new chains. They are rewarded with dogecoin. \n",
    "\n",
    "Dogecoin differs from many other cryptocurrency in that it has no lifetime cap on the number of coins that can be produced. This means it is highly inflationary--by design. This diminishes the coin as a store of value, but increases its use as an actual currency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Market volatililty and the speculative nature of Dogecoin increase risk of owning asset. Since the price of other cryptocurrencies have proven susceptible to the sentiment of its buyers and sellers (citation), the hedfund seeks deeper comprehension of the sentiment of those very buyers and seller, in order to get a better grasp on the market, and possibly leverage that information for profits in the future. This project uses Twitter as a proxy for the buyers and sellers, and analyzes tweets containing the word \"dogecoin.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter API, Tweets, Features, Target\n",
    "\n",
    "Twitter's API has limitations on retrieval. One can only pull tweets from the last 7days, in increments of 180 tweets every 15 minuts, with a cap of 2,600 at one time. Thus, it takes 3 and 1/2 hours to pull accumulate 2,500 tweets. \n",
    "\n",
    "Retweets will be excluded because the Hedgefund is trying to get a sense of dogecoin audience, not the retweets of one bot (which we discovered within our first few pulls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API Request Code\n",
    "```\n",
    "# Authentication\n",
    "consumerKey = creds['API Key']\n",
    "consumerSecret = creds['API Key Secret']\n",
    "accessToken = creds['Access Token']\n",
    "accessTokenSecret = creds['Access Token Secret']\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Pull tweets based on Keyword and Amount (noOfTweet)\n",
    "keyword = input('Please enter keyword or hastag to search: ')\n",
    "noOfTweet = int(input ('Please enter how many tweets to analyze: '))\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=keyword+' -filter:retweets').items(noOfTweet)\n",
    "tweet_list = []\n",
    "tweet_date_list = []\n",
    "for tweet in tweets:\n",
    "    tweet_list.append(tweet.text)\n",
    "    tweet_date_list.append(tweet.created_at)\n",
    "\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "# from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Tweets\n",
    "Because Twitter's API has a limit, we had to pull tweets in batches. Below, we concatenate all these serparate pulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@binance @BinanceChain @dogecoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DogecoinNorway @occupymars42069 @dogeofficial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16 November 2021, 07:36h \\n\\nThe current price...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Give some tip today to a friend or family memb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Jayecane Help everyone by getting them into #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>if you hand me my paycheck and tell me its 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>@BscPitbull @Crypto_Dep @Shibtoken @dogecoin @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>@papacthulu @PabzSantos30 @kidbarrio @dogecoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>@Dogemongo_Daily @RotonKumar17 @dogecoin @Huob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Buy $TZKI from #BitMart. It has a potential as...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4080 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0                     @binance @BinanceChain @dogecoin\n",
       "1    @DogecoinNorway @occupymars42069 @dogeofficial...\n",
       "2    16 November 2021, 07:36h \\n\\nThe current price...\n",
       "3    Give some tip today to a friend or family memb...\n",
       "4    @Jayecane Help everyone by getting them into #...\n",
       "..                                                 ...\n",
       "995  if you hand me my paycheck and tell me its 0.0...\n",
       "996  @BscPitbull @Crypto_Dep @Shibtoken @dogecoin @...\n",
       "997  @papacthulu @PabzSantos30 @kidbarrio @dogecoin...\n",
       "998  @Dogemongo_Daily @RotonKumar17 @dogecoin @Huob...\n",
       "999  Buy $TZKI from #BitMart. It has a potential as...\n",
       "\n",
       "[4080 rows x 1 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.DataFrame(pd.read_csv('data/doge_tweets_111621_1138',index_col=0)['text'])\n",
    "df_2 = pd.DataFrame(pd.read_csv('data/dogecoin2_11_16_21_1pm.csv',index_col=0)['text'])\n",
    "df_3 = pd.DataFrame(pd.read_csv('data/dogecoin_11_17_21_10am.csv', index_col=0)['text'])\n",
    "df_4 = pd.DataFrame(pd.read_csv('data/doge_tweets_111721_1454.csv',index_col=0)['text'])\n",
    "df=pd.concat([df_1,df_2, df_3,df_4])\n",
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function for vectorizing\n",
    "Note, this function removes stopwords and sorts the vectorized  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removed stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, stop_words = 'english',max_df=1.0, min_df=0.01).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n], bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_gram_test(corpus, trained_corpus, ngram_range,n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, stop_words = 'english',max_df=1.0, min_df=0.01).fit(trained_corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n], bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweets\n",
    "We removed punctuation, in order to track similar words. Likewise, we made every word lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove RT\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "# Remove punctuation\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\",\" \",x)\n",
    "# Remove Retweets\n",
    "df[\"clean_text\"] = df.text.map(remove_rt).map(rt)\n",
    "# Lower the case \n",
    "df[\"clean_text\"] = df.clean_text.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.309028</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.351515</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity  subjectivity  sentiment\n",
       "0  0.000000      0.000000        0.0\n",
       "1  0.000000      0.000000        0.0\n",
       "2  0.000000      0.350000        0.0\n",
       "3  0.000000      0.000000        0.0\n",
       "4  0.000000      0.000000        0.0\n",
       "5  0.000000      0.000000        0.0\n",
       "6  0.309028      0.729167        0.0\n",
       "7  0.445455      0.351515        0.0\n",
       "8 -0.400000      0.400000        0.0\n",
       "9  0.000000      0.000000        0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating Negative, Positive, Neutral and Compound values\n",
    "df_probs = pd.DataFrame()\n",
    "df_probs[['polarity', 'subjectivity']] = df['clean_text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in df['text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        df_probs.loc[index, 'sentiment'] = 0\n",
    "    elif pos > neg:\n",
    "        df_probs.loc[index, 'sentiment'] = 1\n",
    "    else:\n",
    "        df_probs.loc[index, 'sentiment'] = 0\n",
    "df_probs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df_pos = df[['text']].copy(deep=True)\n",
    "df_probs.reset_index(drop=True, inplace=True)\n",
    "df_pos['sentiment'] = df_probs['sentiment'].copy(deep=True)\n",
    "df_pos = df_pos[df_pos['sentiment'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove RT\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "# Remove punctuation\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\",\" \",x)\n",
    "# Remove Retweets\n",
    "df_pos[\"clean_text\"] = df_pos.text.map(remove_rt).map(rt).copy(deep=True)\n",
    "# Lower the case \n",
    "df_pos[\"clean_text\"] = df_pos.clean_text.str.lower().copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'earn', 'friend', 'income', 'pasive', 'bitcoin', 'day', '10', 'crypto', 'instagram', 'gift', 'pinterest', 'price', 'support', 'gifts', 'deal', 'like', 'btc', 'current', 'whale', 'alert', 'tx', 'ba', 'good', 'dogecoins', 'swap', 'ethereum', 'eth', 'linkedin', 'shiba', 'a6n4rywnuahiy9nncneozajbgc2opmkqmo', 'tumblr', 'update', 'just', 'shib', 'token', 'great', 'buy', 'amp', 'catecoin', 'worth', 'future', 'nft', 'affiliatemarketing', 'change', 'moon', 'free', 'usd', 'value', 'cryptocurrency', 'shibainu', 'make', 'birthdaygirl', 'dogearmy', 'inu', 'ready', 'hour', 'happy', 'join', 'market', 'binance', 'coins', 'social', 'io', 'mdoge', 'legacy', 'says', 'minute', 'better', 'coming', 'love', 'amazing', 'time', 'awesome', 'tippingtuesday', 'community', 'solana', 'let', 'add', 'miss', 'big', 'addr', 'best', 'empire', 'accept', 'want', 'bnb', 'wallet', 'elon', 'coin', 'looking', 'lol', 'huge', 'yes', 'mxs', 'way', 'ada', 'bwcdeals', 'xrp', 'cash', 'dog', 'floki', 'sure', 'ad', 'meme', 'blogs', 'blogger', 'blog', 'help', 'money', 'cryptocurrencies', 'really', 'hope', 'global', 'week', 'birthday', 'devs', '24', 'ev', 'al']\n"
     ]
    }
   ],
   "source": [
    "positives, pos_vec = get_top_n_gram(df_pos['clean_text'], (1,1), 300)\n",
    "positives = [X[0] for X in positives]\n",
    "positives = [x for x in positives if x not in ['dogecoin','doge','twitter','socialmedia','pintrest','think','today','people','don','facebook','project']]\n",
    "print(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def extract_features(text, top_100):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    positive_scores = list()\n",
    "\n",
    "    for sentence in sent_tokenize(text):\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word.lower() in top_100:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    # since some classifiers you'll use later don't work with negative numbers.\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features['wordcount']\n",
    "\n",
    "features = [\n",
    "    (extract_features(review, positives))\n",
    "    for review in df['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.concat([df['clean_text'], pd.DataFrame(features,columns=['positive_word_count'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>positive_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>babydogepaid airdrops    joining get 100 00...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>dogecoin whale alert    tx  26...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>linkedin  twitter  facebook  instagram  tikto...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>let s look at the top 3 meme based gainers of ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>of bsc      catecoin      the king of meme i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>current  doge price is  0 23776  dogecoin  cry...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>not so mr  wonderful says  dogecoin holders di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>dilshad   ba swap my friend  there is a new ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>saitama shiba i ll have all the ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>dogecoin  doge</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text  positive_word_count\n",
       "2803     babydogepaid airdrops    joining get 100 00...                    4\n",
       "866                   dogecoin whale alert    tx  26...                    4\n",
       "674    linkedin  twitter  facebook  instagram  tikto...                    7\n",
       "468   let s look at the top 3 meme based gainers of ...                    3\n",
       "1299    of bsc      catecoin      the king of meme i...                    3\n",
       "...                                                 ...                  ...\n",
       "2763  current  doge price is  0 23776  dogecoin  cry...                    3\n",
       "905   not so mr  wonderful says  dogecoin holders di...                    1\n",
       "1096    dilshad   ba swap my friend  there is a new ...                    9\n",
       "235               saitama shiba i ll have all the ca...                    1\n",
       "1061                                   dogecoin  doge                      0\n",
       "\n",
       "[2325 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_feat, df_probs['sentiment'],random_state=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat, feat_vec_train = get_top_n_gram(X_train['clean_text'], (1,1), 100)\n",
    "feat, feat_vec_test = get_top_n_gram_test(X_test['clean_text'], X_train['clean_text'], (1,1), 100)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train_vec = pd.concat([X_train['positive_word_count'], pd.DataFrame(feat_vec_train.todense())], axis=1)\n",
    "X_test_vec = pd.concat([X_test['positive_word_count'], pd.DataFrame(feat_vec_test.todense())], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline is the average of the majority class, which is 68% neutral/negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.91397849462366"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1- y_train.sum()/y_train.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72043011 0.67526882 0.70752688 0.6688172  0.70967742]\n",
      "predictions\n",
      "0.0    553\n",
      "1.0    222\n",
      "dtype: int64\n",
      "Accuracy Score: 0.7329032258064516\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec,y_train)\n",
    "\n",
    "#Cross-Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(nb, X_train_vec, y_train))\n",
    "\n",
    "# Predictions\n",
    "y_preds = nb.predict(X_test_vec)\n",
    "df_fin = pd.DataFrame()\n",
    "df_fin['predictions'] = y_preds\n",
    "print(df_fin.value_counts('predictions'))\n",
    "\n",
    "# Metric\n",
    "print('Accuracy Score:',accuracy_score(y_test,df_fin['predictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(nb, X_test_vec, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend using these words and phrases. \n",
    "\n",
    "They mean this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going forward, we would love to offer sentiment analysis over time, along with the quantity of tweets at any given time, to predict price changes. \n",
    "\n",
    "We also would love to anaylze different excahnges. for example, reddit's wall street bets often trades in Robinhood. So doing a comparative sentiment analysis might help with arbitrage of dogecoin or between other currencies. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a3d059f376a9d0551670ac739dcc834dd342b8d7d90019c6bdbef463e084516"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('learn-env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
