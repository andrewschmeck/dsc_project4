{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install Libraries\n",
    "# !pip install textblob\n",
    "# !pip install tweepy\n",
    "# !pip install pycountry\n",
    "# !pip install wordcloud\n",
    "# !pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Problem:\n",
    "\n",
    "* Stakeholder - Hedgefund\n",
    "* Sentiment Analysis with NLP - Identify sentiment through tweets for dogecoin (negative, neutral, positive)\n",
    "* check if twitter sentiment has any correlation for dogecoin price\n",
    "\n",
    "* *Our model is the first academic proof of concept that social media platforms such as Twitter can serve as powerful social signals for predicting price movements in the highly speculative alternative cryptocurrency, or ‚Äúalt-coin,‚Äù market.* [crypto_paper](https://www.frontiersin.org/articles/10.3389/fphy.2019.00098/full)\n",
    "\n",
    "\n",
    "### Dataset:\n",
    "\n",
    "* Twitter API searching for dogecoin (2500 tweets total)\n",
    "\n",
    "### Methods:\n",
    "\n",
    "* Base Model: Sentiment Intensity Analyzer on tweets without preprocessing\n",
    "\n",
    "* Simple Model: Sentiment Intensity Analyzer on tweets with preprocessing\n",
    "    - preprocessing: remove punctuation, lowercase, remove\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/evanjays/.secrets/twitter_creds.json') as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "df_2013 = pd.read_csv('data/judge-1377884607_tweet_product_company.csv', encoding = \"ISO-8859-1\")\n",
    "df_2013.drop(6,inplace=True)\n",
    "\n",
    "df = pd.read_csv('data/doge_tweets.csv',index_col=0)\n",
    "df.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Doge_BUSD: üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT, Follow us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @dream9kk: Elon Musk Thinks Dogecoin Is Sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @MilgateTyler: \"Her hair, long, black and f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...\n",
       "1  RT @Doge_BUSD: üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT, Follow us...\n",
       "2  RT @dream9kk: Elon Musk Thinks Dogecoin Is Sup...\n",
       "3  RT @MilgateTyler: \"Her hair, long, black and f...\n",
       "4  RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Dataset Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Text (RT, Punctuation etc)\n",
    "#Creating new dataframe and new features\n",
    "#Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\",\" \",x)\n",
    "df_2013[\"clean_text\"] = df_2013.tweet_text.map(remove_rt).map(rt)\n",
    "df_2013[\"clean_text\"] = df_2013.clean_text.str.lower()\n",
    "df_2013.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013[['polarity', 'subjectivity']] = df_2013['clean_text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in df_2013['clean_text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        df_2013.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        df_2013.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        df_2013.loc[index, 'sentiment'] = \"neutral\"\n",
    "    df_2013.loc[index, 'neg'] = neg\n",
    "    df_2013.loc[index, 'neu'] = neu\n",
    "    df_2013.loc[index, 'pos'] = pos\n",
    "    df_2013.loc[index, 'compound'] = comp\n",
    "df_2013.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013.is_there_an_emotion_directed_at_a_brand_or_product.value_counts() / 9098 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_num(x):\n",
    "    if x == \"Negative emotion\":\n",
    "        return 0\n",
    "    elif x == \"Positive emotion\":\n",
    "        return 1\n",
    "    elif x == \"No emotion toward brand or product\":\n",
    "        return 2\n",
    "    elif x == \"negative\":\n",
    "        return 0\n",
    "    elif x == \"positive\":\n",
    "        return 1\n",
    "    elif x == \"neutral\":\n",
    "        return 2\n",
    "    elif x == \"I can't tell\":\n",
    "        return np.nan\n",
    "\n",
    "df_2013['true_sent'].dropna(axis=0,inplace=True)\n",
    "\n",
    "df_2013['true_sent'] = df_2013['is_there_an_emotion_directed_at_a_brand_or_product'].map(sent_to_num)\n",
    "df_2013['pred_sent'] = df_2013['sentiment'].map(sent_to_num)\n",
    "\n",
    "df_pred = df_2013.filter(['true_sent','pred_sent']).copy(deep=True)\n",
    "df_pred.dropna(inplace=True)\n",
    "accuracy_score(df_pred['true_sent'],df_pred['pred_sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013.head().iloc[3].tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOGECOIN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication\n",
    "consumerKey = creds['API Key']\n",
    "consumerSecret = creds['API Key Secret']\n",
    "accessToken = creds['Access Token']\n",
    "accessTokenSecret = creds['Access Token Secret']\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "def percentage(part,whole):\n",
    " return 100 * float(part)/float(whole)\n",
    "keyword = input('Please enter keyword or hastag to search: ')\n",
    "noOfTweet = int(input ('Please enter how many tweets to analyze: '))\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=keyword).items(noOfTweet)\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "tweet_date_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "for tweet in tweets:\n",
    " \n",
    " #print(tweet.text)\n",
    "    tweet_list.append(tweet.text)\n",
    "    tweet_date_list.append(tweet.created_at)\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    polarity += analysis.sentiment.polarity\n",
    "\n",
    "    if neg > pos:\n",
    "        negative_list.append(tweet.text)\n",
    "        negative += 1\n",
    "    elif pos > neg:\n",
    "        positive_list.append(tweet.text)\n",
    "        positive += 1\n",
    "    elif pos == neg:\n",
    "        neutral_list.append(tweet.text)\n",
    "        neutral += 1\n",
    "\n",
    "positive = percentage(positive, noOfTweet)\n",
    "negative = percentage(negative, noOfTweet)\n",
    "neutral = percentage(neutral, noOfTweet)\n",
    "polarity = percentage(polarity, noOfTweet)\n",
    "positive = format(positive, '.1f')\n",
    "negative = format(negative, '.1f')\n",
    "neutral = format(neutral, '.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2021, 11, 16, 17, 42, 36, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 35, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 34, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 31, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 28, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 25, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 25, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 24, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 20, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 18, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 16, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 16, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 10, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 10, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 9, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 9, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 9, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 9, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 9, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 9, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 9, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 8, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 6, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 6, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 3, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 42, 2, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 59, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 59, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 58, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 54, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 54, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 53, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 50, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 46, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 44, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 44, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 43, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 39, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 37, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 36, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 34, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 34, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 33, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 33, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 32, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 31, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 31, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 30, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 28, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2021, 11, 16, 17, 41, 28, tzinfo=datetime.timezone.utc)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Tweets (Total, Positive, Negative, Neutral)\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "print('total number: ',len(tweet_list))\n",
    "print('positive number: ',len(positive_list))\n",
    "print('negative number: ', len(negative_list))\n",
    "print('neutral number: ',len(neutral_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating PieCart\n",
    "labels = ['Positive ['+str(positive)+'%]' , 'Neutral ['+str(neutral)+'%]','Negative ['+str(negative)+'%]']\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['yellowgreen', 'blue','red']\n",
    "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
    "plt.style.use('default')\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result for keyword= \"+keyword+\"\" )\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Text (RT, Punctuation etc)\n",
    "#Creating new dataframe and new features\n",
    "tw_list = pd.DataFrame(tweet_list)\n",
    "tw_list[\"text\"] = tw_list[0]\n",
    "#Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\",\" \",x)\n",
    "tw_list[\"text\"] = tw_list.text.map(remove_rt).map(rt)\n",
    "tw_list[\"text\"] = tw_list.text.str.lower()\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Negative, Positive, Neutral and Compound values\n",
    "tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in tw_list['text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        tw_list.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        tw_list.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        tw_list.loc[index, 'sentiment'] = \"neutral\"\n",
    "    tw_list.loc[index, 'neg'] = neg\n",
    "    tw_list.loc[index, 'neu'] = neu\n",
    "    tw_list.loc[index, 'pos'] = pos\n",
    "    tw_list.loc[index, 'compound'] = comp\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new data frames for all sentiments (positive, negative and neutral)\n",
    "tw_list_negative = tw_list[tw_list[\"sentiment\"]==\"negative\"]\n",
    "tw_list_positive = tw_list[tw_list[\"sentiment\"]==\"positive\"]\n",
    "tw_list_neutral = tw_list[tw_list[\"sentiment\"]==\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
    "#Count_values for sentiment\n",
    "count_values_in_column(tw_list,\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for Pie Chart\n",
    "pc = count_values_in_column(tw_list,\"sentiment\")\n",
    "names= pc.index\n",
    "size=pc[\"Percentage\"]\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.7, color='white')\n",
    "plt.pie(size, labels=names, colors=['green','blue','red'])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Create Wordcloud\n",
    "def create_wordcloud(text):\n",
    "    mask = np.array(Image.open(\"cloud.png\"))\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(background_color=\"white\",\n",
    "    mask = mask,\n",
    "    max_words=3000,\n",
    "    stopwords=stopwords,\n",
    "    repeat=True)\n",
    "    wc.generate(str(text))\n",
    "    wc.to_file(\"wc.png\")\n",
    "    print(\"Word Cloud Saved Successfully\")\n",
    "    path=\"wc.png\"\n",
    "    display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating wordcloud for all tweets\n",
    "# create_wordcloud(tw_list[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating wordcloud for positive sentiment\n",
    "# create_wordcloud(tw_list_positive[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating wordcloud for negative sentiment\n",
    "# create_wordcloud(tw_list_negative[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating tweet's length and word count\n",
    "tw_list['text_len'] = tw_list['text'].astype(str).apply(len)\n",
    "tw_list['text_word_count'] = tw_list['text'].apply(lambda x: len(str(x).split()))\n",
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_len.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_word_count.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0‚Äì9]+', '', text)\n",
    "    return text\n",
    "tw_list['punct'] = tw_list['text'].apply(lambda x: remove_punct(x))\n",
    "#Appliyng tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "tw_list['tokenized'] = tw_list['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "#Applying Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: stemming(x))\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove punctuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "tw_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appliyng Countvectorizer\n",
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(tw_list['text'])\n",
    "print('{} Number of reviews has {} words'.format(countVector.shape[0], countVector.shape[1]))\n",
    "# print(countVectorizer.get_feature_names())\n",
    "# 1281 Number of reviews has 2966 words\n",
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Used Words\n",
    "count = pd.DataFrame(count_vect_df.sum())\n",
    "countdf = count.sort_values(0,ascending=False).head(20)\n",
    "countdf[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ngram\n",
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n], bag_of_words\n",
    "#n2_bigram\n",
    "# n2_bigrams = get_top_n_gram(tw_list['text'],(2,2),20)\n",
    "# n2_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n3_trigram\n",
    "n3_trigrams = get_top_n_gram(tw_list['text'],(3,3),20)\n",
    "n3_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL BUILDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>punct</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>nonstop</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...</td>\n",
       "      <td>RT VikingFlki ProTheDoge üí£VIKING FLOKI PRESALE...</td>\n",
       "      <td>[rt, vikingflki, prothedoge, viking, floki, pr...</td>\n",
       "      <td>[rt, vikingflki, prothedoge, viking, floki, pr...</td>\n",
       "      <td>[rt, vikingflki, prothedog, vike, floki, presa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Doge_BUSD: üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT, Follow us...</td>\n",
       "      <td>RT DogeBUSD üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT Follow us and...</td>\n",
       "      <td>[rt, dogebusd, giveaway, rt, follow, us, and, ...</td>\n",
       "      <td>[rt, dogebusd, giveaway, rt, follow, us, tag, ...</td>\n",
       "      <td>[rt, dogebusd, giveaway, rt, follow, us, tag, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @dream9kk: Elon Musk Thinks Dogecoin Is Sup...</td>\n",
       "      <td>RT dreamkk Elon Musk Thinks Dogecoin Is Superi...</td>\n",
       "      <td>[rt, dreamkk, elon, musk, thinks, dogecoin, is...</td>\n",
       "      <td>[rt, dreamkk, elon, musk, thinks, dogecoin, su...</td>\n",
       "      <td>[rt, dreamkk, elon, musk, think, dogecoin, sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @MilgateTyler: \"Her hair, long, black and f...</td>\n",
       "      <td>RT MilgateTyler Her hair long black and flowin...</td>\n",
       "      <td>[rt, milgatetyler, her, hair, long, black, and...</td>\n",
       "      <td>[rt, milgatetyler, hair, long, black, flowing,...</td>\n",
       "      <td>[rt, milgatetyl, hair, long, black, flow, grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...</td>\n",
       "      <td>RT VikingFlki ProTheDoge üí£VIKING FLOKI PRESALE...</td>\n",
       "      <td>[rt, vikingflki, prothedoge, viking, floki, pr...</td>\n",
       "      <td>[rt, vikingflki, prothedoge, viking, floki, pr...</td>\n",
       "      <td>[rt, vikingflki, prothedog, vike, floki, presa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...   \n",
       "1  RT @Doge_BUSD: üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT, Follow us...   \n",
       "2  RT @dream9kk: Elon Musk Thinks Dogecoin Is Sup...   \n",
       "3  RT @MilgateTyler: \"Her hair, long, black and f...   \n",
       "4  RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...   \n",
       "\n",
       "                                               punct  \\\n",
       "0  RT VikingFlki ProTheDoge üí£VIKING FLOKI PRESALE...   \n",
       "1  RT DogeBUSD üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT Follow us and...   \n",
       "2  RT dreamkk Elon Musk Thinks Dogecoin Is Superi...   \n",
       "3  RT MilgateTyler Her hair long black and flowin...   \n",
       "4  RT VikingFlki ProTheDoge üí£VIKING FLOKI PRESALE...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [rt, vikingflki, prothedoge, viking, floki, pr...   \n",
       "1  [rt, dogebusd, giveaway, rt, follow, us, and, ...   \n",
       "2  [rt, dreamkk, elon, musk, thinks, dogecoin, is...   \n",
       "3  [rt, milgatetyler, her, hair, long, black, and...   \n",
       "4  [rt, vikingflki, prothedoge, viking, floki, pr...   \n",
       "\n",
       "                                             nonstop  \\\n",
       "0  [rt, vikingflki, prothedoge, viking, floki, pr...   \n",
       "1  [rt, dogebusd, giveaway, rt, follow, us, tag, ...   \n",
       "2  [rt, dreamkk, elon, musk, thinks, dogecoin, su...   \n",
       "3  [rt, milgatetyler, hair, long, black, flowing,...   \n",
       "4  [rt, vikingflki, prothedoge, viking, floki, pr...   \n",
       "\n",
       "                                             stemmed  \n",
       "0  [rt, vikingflki, prothedog, vike, floki, presa...  \n",
       "1  [rt, dogebusd, giveaway, rt, follow, us, tag, ...  \n",
       "2  [rt, dreamkk, elon, musk, think, dogecoin, sup...  \n",
       "3  [rt, milgatetyl, hair, long, black, flow, grea...  \n",
       "4  [rt, vikingflki, prothedog, vike, floki, presa...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0‚Äì9]+', '', text)\n",
    "    return text\n",
    "df['punct'] = df['text'].apply(lambda x: remove_punct(x))\n",
    "#Appliyng tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "df['tokenized'] = df['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "df['nonstop'] = df['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "#Applying Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "df['stemmed'] = df['nonstop'].apply(lambda x: stemming(x))\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove punctuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 14999)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2105006958651209"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_list,vec = get_top_n_gram(df['text'], (1,3),500)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "kmeans.fit(vec)\n",
    "\n",
    "y_preds = kmeans.predict(vec)\n",
    "\n",
    "df_fin = df.copy(deep=True)\n",
    "\n",
    "df_fin['predictions'] = y_preds\n",
    "\n",
    "df_fin.value_counts('predictions')\n",
    "\n",
    "silhouette_score(vec,y_preds)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a3d059f376a9d0551670ac739dcc834dd342b8d7d90019c6bdbef463e084516"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('learn-env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
