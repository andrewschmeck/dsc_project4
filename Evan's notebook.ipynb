{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install Libraries\n",
    "# !pip install textblob\n",
    "# !pip install tweepy\n",
    "# !pip install pycountry\n",
    "# !pip install wordcloud\n",
    "# !pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Problem:\n",
    "\n",
    "* Stakeholder - Hedgefund\n",
    "* Sentiment Analysis with NLP - Identify sentiment through tweets for dogecoin (negative, neutral, positive)\n",
    "* check if twitter sentiment has any correlation for dogecoin price\n",
    "\n",
    "* *Our model is the first academic proof of concept that social media platforms such as Twitter can serve as powerful social signals for predicting price movements in the highly speculative alternative cryptocurrency, or ‚Äúalt-coin,‚Äù market.* [crypto_paper](https://www.frontiersin.org/articles/10.3389/fphy.2019.00098/full)\n",
    "\n",
    "\n",
    "### Dataset:\n",
    "\n",
    "* Twitter API searching for dogecoin (2500 tweets total)\n",
    "\n",
    "### Methods:\n",
    "\n",
    "* Base Model: Sentiment Intensity Analyzer on tweets without preprocessing\n",
    "\n",
    "* Simple Model: Sentiment Intensity Analyzer on tweets with preprocessing\n",
    "    - preprocessing: remove punctuation, lowercase, remove\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/evanjays/.secrets/twitter_creds.json') as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "df_2013 = pd.read_csv('data/judge-1377884607_tweet_product_company.csv', encoding = \"ISO-8859-1\")\n",
    "df_2013.drop(6,inplace=True)\n",
    "\n",
    "df = pd.read_csv('data/doge_tweets.csv',index_col=0)\n",
    "df.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Doge_BUSD: üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT, Follow us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @dream9kk: Elon Musk Thinks Dogecoin Is Sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @MilgateTyler: \"Her hair, long, black and f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...\n",
       "1  RT @Doge_BUSD: üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT, Follow us...\n",
       "2  RT @dream9kk: Elon Musk Thinks Dogecoin Is Sup...\n",
       "3  RT @MilgateTyler: \"Her hair, long, black and f...\n",
       "4  RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Dataset Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Text (RT, Punctuation etc)\n",
    "#Creating new dataframe and new features\n",
    "#Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\",\" \",x)\n",
    "df_2013[\"clean_text\"] = df_2013.tweet_text.map(remove_rt).map(rt)\n",
    "df_2013[\"clean_text\"] = df_2013.clean_text.str.lower()\n",
    "df_2013.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013[['polarity', 'subjectivity']] = df_2013['clean_text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in df_2013['clean_text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        df_2013.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        df_2013.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        df_2013.loc[index, 'sentiment'] = \"neutral\"\n",
    "    df_2013.loc[index, 'neg'] = neg\n",
    "    df_2013.loc[index, 'neu'] = neu\n",
    "    df_2013.loc[index, 'pos'] = pos\n",
    "    df_2013.loc[index, 'compound'] = comp\n",
    "df_2013.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013.is_there_an_emotion_directed_at_a_brand_or_product.value_counts() / 9098 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_num(x):\n",
    "    if x == \"Negative emotion\":\n",
    "        return 0\n",
    "    elif x == \"Positive emotion\":\n",
    "        return 1\n",
    "    elif x == \"No emotion toward brand or product\":\n",
    "        return 2\n",
    "    elif x == \"negative\":\n",
    "        return 0\n",
    "    elif x == \"positive\":\n",
    "        return 1\n",
    "    elif x == \"neutral\":\n",
    "        return 2\n",
    "    elif x == \"I can't tell\":\n",
    "        return np.nan\n",
    "\n",
    "df_2013['true_sent'].dropna(axis=0,inplace=True)\n",
    "\n",
    "df_2013['true_sent'] = df_2013['is_there_an_emotion_directed_at_a_brand_or_product'].map(sent_to_num)\n",
    "df_2013['pred_sent'] = df_2013['sentiment'].map(sent_to_num)\n",
    "\n",
    "df_pred = df_2013.filter(['true_sent','pred_sent']).copy(deep=True)\n",
    "df_pred.dropna(inplace=True)\n",
    "accuracy_score(df_pred['true_sent'],df_pred['pred_sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013.head().iloc[3].tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOGECOIN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication\n",
    "consumerKey = creds['API Key']\n",
    "consumerSecret = creds['API Key Secret']\n",
    "accessToken = creds['Access Token']\n",
    "accessTokenSecret = creds['Access Token Secret']\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TooManyRequests",
     "evalue": "429 Too Many Requests\n88 - Rate limit exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-7c030f304786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnegative_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpositive_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m  \u001b[0;31m#print(tweet.text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             model = ModelParser().parse(\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpagination_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36msearch_tweets\u001b[0;34m(self, q, **kwargs)\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdeveloper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \"\"\"\n\u001b[0;32m-> 1268\u001b[0;31m         return self.request(\n\u001b[0m\u001b[1;32m   1269\u001b[0m             'GET', 'search/tweets', endpoint_parameters=(\n\u001b[1;32m   1270\u001b[0m                 \u001b[0;34m'q'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geocode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lang'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'locale'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'result_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m429\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTooManyRequests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTwitterServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTooManyRequests\u001b[0m: 429 Too Many Requests\n88 - Rate limit exceeded"
     ]
    }
   ],
   "source": [
    "#Sentiment Analysis\n",
    "def percentage(part,whole):\n",
    " return 100 * float(part)/float(whole)\n",
    "keyword = input('Please enter keyword or hastag to search: ')\n",
    "noOfTweet = int(input ('Please enter how many tweets to analyze: '))\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=keyword+' -filter:retweets').items(noOfTweet)\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "tweet_date_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "for tweet in tweets:\n",
    " \n",
    " #print(tweet.text)\n",
    "    tweet_list.append(tweet.text)\n",
    "    tweet_date_list.append(tweet.created_at)\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    polarity += analysis.sentiment.polarity\n",
    "\n",
    "    if neg > pos:\n",
    "        negative_list.append(tweet.text)\n",
    "        negative += 1\n",
    "    elif pos > neg:\n",
    "        positive_list.append(tweet.text)\n",
    "        positive += 1\n",
    "    elif pos == neg:\n",
    "        neutral_list.append(tweet.text)\n",
    "        neutral += 1\n",
    "\n",
    "positive = percentage(positive, noOfTweet)\n",
    "negative = percentage(negative, noOfTweet)\n",
    "neutral = percentage(neutral, noOfTweet)\n",
    "polarity = percentage(polarity, noOfTweet)\n",
    "positive = format(positive, '.1f')\n",
    "negative = format(negative, '.1f')\n",
    "neutral = format(neutral, '.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Tweets (Total, Positive, Negative, Neutral)\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "print('total number: ',len(tweet_list))\n",
    "print('positive number: ',len(positive_list))\n",
    "print('negative number: ', len(negative_list))\n",
    "print('neutral number: ',len(neutral_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating PieCart\n",
    "labels = ['Positive ['+str(positive)+'%]' , 'Neutral ['+str(neutral)+'%]','Negative ['+str(negative)+'%]']\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['yellowgreen', 'blue','red']\n",
    "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
    "plt.style.use('default')\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result for keyword= \"+keyword+\"\" )\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Text (RT, Punctuation etc)\n",
    "#Creating new dataframe and new features\n",
    "tw_list = pd.DataFrame(tweet_list)\n",
    "tw_list[\"text\"] = tw_list[0]\n",
    "#Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\",\" \",x)\n",
    "tw_list[\"text\"] = tw_list.text.map(remove_rt).map(rt)\n",
    "tw_list[\"text\"] = tw_list.text.str.lower()\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Negative, Positive, Neutral and Compound values\n",
    "tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in tw_list['text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        tw_list.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        tw_list.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        tw_list.loc[index, 'sentiment'] = \"neutral\"\n",
    "    tw_list.loc[index, 'neg'] = neg\n",
    "    tw_list.loc[index, 'neu'] = neu\n",
    "    tw_list.loc[index, 'pos'] = pos\n",
    "    tw_list.loc[index, 'compound'] = comp\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new data frames for all sentiments (positive, negative and neutral)\n",
    "tw_list_negative = tw_list[tw_list[\"sentiment\"]==\"negative\"]\n",
    "tw_list_positive = tw_list[tw_list[\"sentiment\"]==\"positive\"]\n",
    "tw_list_neutral = tw_list[tw_list[\"sentiment\"]==\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
    "#Count_values for sentiment\n",
    "count_values_in_column(tw_list,\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for Pie Chart\n",
    "pc = count_values_in_column(tw_list,\"sentiment\")\n",
    "names= pc.index\n",
    "size=pc[\"Percentage\"]\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.7, color='white')\n",
    "plt.pie(size, labels=names, colors=['green','blue','red'])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Create Wordcloud\n",
    "def create_wordcloud(text):\n",
    "    mask = np.array(Image.open(\"cloud.png\"))\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(background_color=\"white\",\n",
    "    mask = mask,\n",
    "    max_words=3000,\n",
    "    stopwords=stopwords,\n",
    "    repeat=True)\n",
    "    wc.generate(str(text))\n",
    "    wc.to_file(\"wc.png\")\n",
    "    print(\"Word Cloud Saved Successfully\")\n",
    "    path=\"wc.png\"\n",
    "    display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating wordcloud for all tweets\n",
    "# create_wordcloud(tw_list[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating wordcloud for positive sentiment\n",
    "# create_wordcloud(tw_list_positive[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating wordcloud for negative sentiment\n",
    "# create_wordcloud(tw_list_negative[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating tweet's length and word count\n",
    "tw_list['text_len'] = tw_list['text'].astype(str).apply(len)\n",
    "tw_list['text_word_count'] = tw_list['text'].apply(lambda x: len(str(x).split()))\n",
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_len.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_word_count.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0‚Äì9]+', '', text)\n",
    "    return text\n",
    "tw_list['punct'] = tw_list['text'].apply(lambda x: remove_punct(x))\n",
    "#Appliyng tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "tw_list['tokenized'] = tw_list['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "#Applying Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: stemming(x))\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove punctuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "tw_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appliyng Countvectorizer\n",
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(tw_list['text'])\n",
    "print('{} Number of reviews has {} words'.format(countVector.shape[0], countVector.shape[1]))\n",
    "# print(countVectorizer.get_feature_names())\n",
    "# 1281 Number of reviews has 2966 words\n",
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Used Words\n",
    "count = pd.DataFrame(count_vect_df.sum())\n",
    "countdf = count.sort_values(0,ascending=False).head(20)\n",
    "countdf[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ngram\n",
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n], bag_of_words\n",
    "#n2_bigram\n",
    "# n2_bigrams = get_top_n_gram(tw_list['text'],(2,2),20)\n",
    "# n2_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n3_trigram\n",
    "n3_trigrams = get_top_n_gram(tw_list['text'],(3,3),20)\n",
    "n3_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL BUILDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>punct</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>nonstop</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...</td>\n",
       "      <td>RT VikingFlki ProTheDoge üí£VIKING FLOKI PRESALE...</td>\n",
       "      <td>[rt, vikingflki, prothedoge, viking, floki, pr...</td>\n",
       "      <td>[rt, vikingflki, prothedoge, viking, floki, pr...</td>\n",
       "      <td>[rt, vikingflki, prothedog, vike, floki, presa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Doge_BUSD: üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT, Follow us...</td>\n",
       "      <td>RT DogeBUSD üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT Follow us and...</td>\n",
       "      <td>[rt, dogebusd, giveaway, rt, follow, us, and, ...</td>\n",
       "      <td>[rt, dogebusd, giveaway, rt, follow, us, tag, ...</td>\n",
       "      <td>[rt, dogebusd, giveaway, rt, follow, us, tag, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @dream9kk: Elon Musk Thinks Dogecoin Is Sup...</td>\n",
       "      <td>RT dreamkk Elon Musk Thinks Dogecoin Is Superi...</td>\n",
       "      <td>[rt, dreamkk, elon, musk, thinks, dogecoin, is...</td>\n",
       "      <td>[rt, dreamkk, elon, musk, thinks, dogecoin, su...</td>\n",
       "      <td>[rt, dreamkk, elon, musk, think, dogecoin, sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @MilgateTyler: \"Her hair, long, black and f...</td>\n",
       "      <td>RT MilgateTyler Her hair long black and flowin...</td>\n",
       "      <td>[rt, milgatetyler, her, hair, long, black, and...</td>\n",
       "      <td>[rt, milgatetyler, hair, long, black, flowing,...</td>\n",
       "      <td>[rt, milgatetyl, hair, long, black, flow, grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...</td>\n",
       "      <td>RT VikingFlki ProTheDoge üí£VIKING FLOKI PRESALE...</td>\n",
       "      <td>[rt, vikingflki, prothedoge, viking, floki, pr...</td>\n",
       "      <td>[rt, vikingflki, prothedoge, viking, floki, pr...</td>\n",
       "      <td>[rt, vikingflki, prothedog, vike, floki, presa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...   \n",
       "1  RT @Doge_BUSD: üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT, Follow us...   \n",
       "2  RT @dream9kk: Elon Musk Thinks Dogecoin Is Sup...   \n",
       "3  RT @MilgateTyler: \"Her hair, long, black and f...   \n",
       "4  RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...   \n",
       "\n",
       "                                               punct  \\\n",
       "0  RT VikingFlki ProTheDoge üí£VIKING FLOKI PRESALE...   \n",
       "1  RT DogeBUSD üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT Follow us and...   \n",
       "2  RT dreamkk Elon Musk Thinks Dogecoin Is Superi...   \n",
       "3  RT MilgateTyler Her hair long black and flowin...   \n",
       "4  RT VikingFlki ProTheDoge üí£VIKING FLOKI PRESALE...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [rt, vikingflki, prothedoge, viking, floki, pr...   \n",
       "1  [rt, dogebusd, giveaway, rt, follow, us, and, ...   \n",
       "2  [rt, dreamkk, elon, musk, thinks, dogecoin, is...   \n",
       "3  [rt, milgatetyler, her, hair, long, black, and...   \n",
       "4  [rt, vikingflki, prothedoge, viking, floki, pr...   \n",
       "\n",
       "                                             nonstop  \\\n",
       "0  [rt, vikingflki, prothedoge, viking, floki, pr...   \n",
       "1  [rt, dogebusd, giveaway, rt, follow, us, tag, ...   \n",
       "2  [rt, dreamkk, elon, musk, thinks, dogecoin, su...   \n",
       "3  [rt, milgatetyler, hair, long, black, flowing,...   \n",
       "4  [rt, vikingflki, prothedoge, viking, floki, pr...   \n",
       "\n",
       "                                             stemmed  \n",
       "0  [rt, vikingflki, prothedog, vike, floki, presa...  \n",
       "1  [rt, dogebusd, giveaway, rt, follow, us, tag, ...  \n",
       "2  [rt, dreamkk, elon, musk, think, dogecoin, sup...  \n",
       "3  [rt, milgatetyl, hair, long, black, flow, grea...  \n",
       "4  [rt, vikingflki, prothedog, vike, floki, presa...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0‚Äì9]+', '', text)\n",
    "    return text\n",
    "df['punct'] = df['text'].apply(lambda x: remove_punct(x))\n",
    "#Appliyng tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "df['tokenized'] = df['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "df['nonstop'] = df['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "#Applying Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "df['stemmed'] = df['nonstop'].apply(lambda x: stemming(x))\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove punctuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 14999)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2105006958651209"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_list,vec = get_top_n_gram(df['text'], (1,3),500)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca_vec = pca.fit_transform(vec.todense())\n",
    "\n",
    "kmeans.fit(pca_vec)\n",
    "\n",
    "y_preds = kmeans.predict(pca_vec)\n",
    "\n",
    "df_fin = df.copy(deep=True)\n",
    "\n",
    "df_fin['predictions'] = y_preds\n",
    "\n",
    "df_fin.value_counts('predictions')\n",
    "\n",
    "print(silhouette_score(vec,y_preds))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fa0e9e99520>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,10))\n",
    "\n",
    "ax.scatter(x=pca_vec[:,0],y=pca_vec[:,1],c=y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1779\n",
       "1     585\n",
       "2     136\n",
       "Name: predictions, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "382     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "386     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "400     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "401     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "422     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "427     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "439     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "444     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "447     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "448     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "457     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "463     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "474     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "505     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "511     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "543     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "580     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "593     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "599     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "607     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "612     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "617     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "629     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "660     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "672     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "676     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "721     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "733     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "767     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "798     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "801     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "857     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "860     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "892     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "896     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "900     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "905     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "915     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "948     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "959     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "982     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "984     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "995     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "999     RT @FlokiDoge_: What an amazing launch! almost...\n",
       "1020    RT @FlokiDoge_: What an amazing launch! almost...\n",
       "1025    RT @FlokiDoge_: What an amazing launch! almost...\n",
       "1040    RT @FlokiDoge_: What an amazing launch! almost...\n",
       "1061    RT @FlokiDoge_: What an amazing launch! almost...\n",
       "1076    RT @FlokiDoge_: What an amazing launch! almost...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin[df_fin.predictions == 2].head(50)['text']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a3d059f376a9d0551670ac739dcc834dd342b8d7d90019c6bdbef463e084516"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('learn-env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
