{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install Libraries\n",
    "# !pip install textblob\n",
    "# !pip install tweepy\n",
    "# !pip install pycountry\n",
    "# !pip install wordcloud\n",
    "# !pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Problem:\n",
    "\n",
    "* Stakeholder - Hedgefund\n",
    "* Sentiment Analysis with NLP - Identify sentiment through tweets for dogecoin (negative, neutral, positive)\n",
    "* check if twitter sentiment has any correlation for dogecoin price\n",
    "\n",
    "* *Our model is the first academic proof of concept that social media platforms such as Twitter can serve as powerful social signals for predicting price movements in the highly speculative alternative cryptocurrency, or ‚Äúalt-coin,‚Äù market.* [crypto_paper](https://www.frontiersin.org/articles/10.3389/fphy.2019.00098/full)\n",
    "\n",
    "\n",
    "### Dataset:\n",
    "\n",
    "* Twitter API searching for dogecoin (2500 tweets total)\n",
    "\n",
    "### Methods:\n",
    "\n",
    "* Base Model: Sentiment Intensity Analyzer on tweets without preprocessing\n",
    "\n",
    "* Simple Model: Sentiment Intensity Analyzer on tweets with preprocessing\n",
    "    - preprocessing: remove punctuation, lowercase, remove\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@binance @BinanceChain @dogecoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DogecoinNorway @occupymars42069 @dogeofficial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16 November 2021, 07:36h \\n\\nThe current price...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Give some tip today to a friend or family memb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Jayecane Help everyone by getting them into #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>#linkedin #twitter #facebook #instagram #dogec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>@mario_snajder @shibworld My friend, there is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>@airdropcryt_ico @shibworld My friend, there i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>@jisungie_02 @shibworld My friend, there is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>Today Crypto Marketüî™üî™üî™üî™ @TechnoCrazy8 @kapil_s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2492 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0                      @binance @BinanceChain @dogecoin\n",
       "1     @DogecoinNorway @occupymars42069 @dogeofficial...\n",
       "2     16 November 2021, 07:36h \\n\\nThe current price...\n",
       "3     Give some tip today to a friend or family memb...\n",
       "4     @Jayecane Help everyone by getting them into #...\n",
       "...                                                 ...\n",
       "2495  #linkedin #twitter #facebook #instagram #dogec...\n",
       "2496  @mario_snajder @shibworld My friend, there is ...\n",
       "2497  @airdropcryt_ico @shibworld My friend, there i...\n",
       "2498  @jisungie_02 @shibworld My friend, there is a ...\n",
       "2499  Today Crypto Marketüî™üî™üî™üî™ @TechnoCrazy8 @kapil_s...\n",
       "\n",
       "[2492 rows x 1 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/evanjays/.secrets/twitter_creds.json') as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "df_2013 = pd.read_csv('data/judge-1377884607_tweet_product_company.csv', encoding = \"ISO-8859-1\")\n",
    "df_2013.drop(6,inplace=True)\n",
    "\n",
    "df_1 = pd.DataFrame(pd.read_csv('data/doge_tweets_111621_1138',index_col=0)['text'])\n",
    "df_2 = pd.DataFrame(pd.read_csv('data/dogecoin2_11_16_21_1pm.csv',index_col=0)['text'])\n",
    "df=pd.concat([df_1,df_2])\n",
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Doge_BUSD: üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT, Follow us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @dream9kk: Elon Musk Thinks Dogecoin Is Sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @MilgateTyler: \"Her hair, long, black and f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...\n",
       "1  RT @Doge_BUSD: üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT, Follow us...\n",
       "2  RT @dream9kk: Elon Musk Thinks Dogecoin Is Sup...\n",
       "3  RT @MilgateTyler: \"Her hair, long, black and f...\n",
       "4  RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Dataset Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Text (RT, Punctuation etc)\n",
    "#Creating new dataframe and new features\n",
    "#Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\",\" \",x)\n",
    "df_2013[\"clean_text\"] = df_2013.tweet_text.map(remove_rt).map(rt)\n",
    "df_2013[\"clean_text\"] = df_2013.clean_text.str.lower()\n",
    "df_2013.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013[['polarity', 'subjectivity']] = df_2013['clean_text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in df_2013['clean_text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        df_2013.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        df_2013.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        df_2013.loc[index, 'sentiment'] = \"neutral\"\n",
    "    df_2013.loc[index, 'neg'] = neg\n",
    "    df_2013.loc[index, 'neu'] = neu\n",
    "    df_2013.loc[index, 'pos'] = pos\n",
    "    df_2013.loc[index, 'compound'] = comp\n",
    "df_2013.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013.is_there_an_emotion_directed_at_a_brand_or_product.value_counts() / 9098 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_num(x):\n",
    "    if x == \"Negative emotion\":\n",
    "        return 0\n",
    "    elif x == \"Positive emotion\":\n",
    "        return 1\n",
    "    elif x == \"No emotion toward brand or product\":\n",
    "        return 2\n",
    "    elif x == \"negative\":\n",
    "        return 0\n",
    "    elif x == \"positive\":\n",
    "        return 1\n",
    "    elif x == \"neutral\":\n",
    "        return 2\n",
    "    elif x == \"I can't tell\":\n",
    "        return np.nan\n",
    "\n",
    "df_2013['true_sent'].dropna(axis=0,inplace=True)\n",
    "\n",
    "df_2013['true_sent'] = df_2013['is_there_an_emotion_directed_at_a_brand_or_product'].map(sent_to_num)\n",
    "df_2013['pred_sent'] = df_2013['sentiment'].map(sent_to_num)\n",
    "\n",
    "df_pred = df_2013.filter(['true_sent','pred_sent']).copy(deep=True)\n",
    "df_pred.dropna(inplace=True)\n",
    "accuracy_score(df_pred['true_sent'],df_pred['pred_sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013.head().iloc[3].tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOGECOIN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication\n",
    "consumerKey = creds['API Key']\n",
    "consumerSecret = creds['API Key Secret']\n",
    "accessToken = creds['Access Token']\n",
    "accessTokenSecret = creds['Access Token Secret']\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "def percentage(part,whole):\n",
    " return 100 * float(part)/float(whole)\n",
    "keyword = input('Please enter keyword or hastag to search: ')\n",
    "noOfTweet = int(input ('Please enter how many tweets to analyze: '))\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=keyword+' -filter:retweets').items(noOfTweet)\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "tweet_date_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "for tweet in tweets:\n",
    " \n",
    " #print(tweet.text)\n",
    "    tweet_list.append(tweet.text)\n",
    "    tweet_date_list.append(tweet.created_at)\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    polarity += analysis.sentiment.polarity\n",
    "\n",
    "    if neg > pos:\n",
    "        negative_list.append(tweet.text)\n",
    "        negative += 1\n",
    "    elif pos > neg:\n",
    "        positive_list.append(tweet.text)\n",
    "        positive += 1\n",
    "    elif pos == neg:\n",
    "        neutral_list.append(tweet.text)\n",
    "        neutral += 1\n",
    "\n",
    "positive = percentage(positive, noOfTweet)\n",
    "negative = percentage(negative, noOfTweet)\n",
    "neutral = percentage(neutral, noOfTweet)\n",
    "polarity = percentage(polarity, noOfTweet)\n",
    "positive = format(positive, '.1f')\n",
    "negative = format(negative, '.1f')\n",
    "neutral = format(neutral, '.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number:  500\n",
      "positive number:  134\n",
      "negative number:  26\n",
      "neutral number:  340\n"
     ]
    }
   ],
   "source": [
    "#Number of Tweets (Total, Positive, Negative, Neutral)\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "print('total number: ',len(tweet_list))\n",
    "print('positive number: ',len(positive_list))\n",
    "print('negative number: ', len(negative_list))\n",
    "print('neutral number: ',len(neutral_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating PieCart\n",
    "labels = ['Positive ['+str(positive)+'%]' , 'Neutral ['+str(neutral)+'%]','Negative ['+str(negative)+'%]']\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['yellowgreen', 'blue','red']\n",
    "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
    "plt.style.use('default')\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result for keyword= \"+keyword+\"\" )\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@binance @BinanceChain @dogecoin</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DogecoinNorway @occupymars42069 @dogeofficial...</td>\n",
       "      <td>oh  dogecoin i luv you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16 November 2021, 07:36h \\n\\nThe current price...</td>\n",
       "      <td>16 november 2021  07 36h   the current price o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Give some tip today to a friend or family memb...</td>\n",
       "      <td>give some tip today to a friend or family memb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Jayecane Help everyone by getting them into #...</td>\n",
       "      <td>help everyone by getting them into  dogecoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://t.co/LvAx1Xb1sd #Messi #Binance #Bitco...</td>\n",
       "      <td>messi  binance  bitcoin  eysevgili  sezaika...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@NickBalazs I agree! The original dogecoin com...</td>\n",
       "      <td>i agree  the original dogecoin community was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@dogecoin_empire Haha I don't even have money ...</td>\n",
       "      <td>empire haha i don t even have money to buy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@askthedr @elonmusk @dogecoin @ProTheDoge @Mat...</td>\n",
       "      <td>once  shibarium launches  it s gam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#webtalk #bitcoin #tumblr #twitter #facebook #...</td>\n",
       "      <td>webtalk  bitcoin  tumblr  twitter  facebook  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0                   @binance @BinanceChain @dogecoin   \n",
       "1  @DogecoinNorway @occupymars42069 @dogeofficial...   \n",
       "2  16 November 2021, 07:36h \\n\\nThe current price...   \n",
       "3  Give some tip today to a friend or family memb...   \n",
       "4  @Jayecane Help everyone by getting them into #...   \n",
       "5  https://t.co/LvAx1Xb1sd #Messi #Binance #Bitco...   \n",
       "6  @NickBalazs I agree! The original dogecoin com...   \n",
       "7  @dogecoin_empire Haha I don't even have money ...   \n",
       "8  @askthedr @elonmusk @dogecoin @ProTheDoge @Mat...   \n",
       "9  #webtalk #bitcoin #tumblr #twitter #facebook #...   \n",
       "\n",
       "                                                text  \n",
       "0                                                     \n",
       "1                        oh  dogecoin i luv you       \n",
       "2  16 november 2021  07 36h   the current price o...  \n",
       "3  give some tip today to a friend or family memb...  \n",
       "4    help everyone by getting them into  dogecoin...  \n",
       "5     messi  binance  bitcoin  eysevgili  sezaika...  \n",
       "6    i agree  the original dogecoin community was...  \n",
       "7    empire haha i don t even have money to buy a...  \n",
       "8              once  shibarium launches  it s gam...  \n",
       "9   webtalk  bitcoin  tumblr  twitter  facebook  ...  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning Text (RT, Punctuation etc)\n",
    "#Creating new dataframe and new features\n",
    "tw_list = pd.DataFrame(tweet_list)\n",
    "tw_list[\"text\"] = tw_list[0]\n",
    "\n",
    "# tw_list.to_csv('data/doge_tweets_111621_1138')\n",
    "#Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\",\" \",x)\n",
    "tw_list[\"text\"] = tw_list.text.map(remove_rt).map(rt)\n",
    "tw_list[\"text\"] = tw_list.text.str.lower()\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Negative, Positive, Neutral and Compound values\n",
    "tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in tw_list['text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        tw_list.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        tw_list.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        tw_list.loc[index, 'sentiment'] = \"neutral\"\n",
    "    tw_list.loc[index, 'neg'] = neg\n",
    "    tw_list.loc[index, 'neu'] = neu\n",
    "    tw_list.loc[index, 'pos'] = pos\n",
    "    tw_list.loc[index, 'compound'] = comp\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new data frames for all sentiments (positive, negative and neutral)\n",
    "tw_list_negative = tw_list[tw_list[\"sentiment\"]==\"negative\"]\n",
    "tw_list_positive = tw_list[tw_list[\"sentiment\"]==\"positive\"]\n",
    "tw_list_neutral = tw_list[tw_list[\"sentiment\"]==\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
    "#Count_values for sentiment\n",
    "count_values_in_column(tw_list,\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for Pie Chart\n",
    "pc = count_values_in_column(tw_list,\"sentiment\")\n",
    "names= pc.index\n",
    "size=pc[\"Percentage\"]\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.7, color='white')\n",
    "plt.pie(size, labels=names, colors=['green','blue','red'])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Create Wordcloud\n",
    "def create_wordcloud(text):\n",
    "    mask = np.array(Image.open(\"cloud.png\"))\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(background_color=\"white\",\n",
    "    mask = mask,\n",
    "    max_words=3000,\n",
    "    stopwords=stopwords,\n",
    "    repeat=True)\n",
    "    wc.generate(str(text))\n",
    "    wc.to_file(\"wc.png\")\n",
    "    print(\"Word Cloud Saved Successfully\")\n",
    "    path=\"wc.png\"\n",
    "    display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating wordcloud for all tweets\n",
    "# create_wordcloud(tw_list[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating wordcloud for positive sentiment\n",
    "# create_wordcloud(tw_list_positive[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating wordcloud for negative sentiment\n",
    "# create_wordcloud(tw_list_negative[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating tweet's length and word count\n",
    "tw_list['text_len'] = tw_list['text'].astype(str).apply(len)\n",
    "tw_list['text_word_count'] = tw_list['text'].apply(lambda x: len(str(x).split()))\n",
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_len.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_word_count.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0‚Äì9]+', '', text)\n",
    "    return text\n",
    "tw_list['punct'] = tw_list['text'].apply(lambda x: remove_punct(x))\n",
    "#Appliyng tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "tw_list['tokenized'] = tw_list['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "#Applying Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: stemming(x))\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove punctuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "tw_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appliyng Countvectorizer\n",
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(tw_list['text'])\n",
    "print('{} Number of reviews has {} words'.format(countVector.shape[0], countVector.shape[1]))\n",
    "# print(countVectorizer.get_feature_names())\n",
    "# 1281 Number of reviews has 2966 words\n",
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Used Words\n",
    "count = pd.DataFrame(count_vect_df.sum())\n",
    "countdf = count.sort_values(0,ascending=False).head(20)\n",
    "countdf[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ngram\n",
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n], bag_of_words\n",
    "#n2_bigram\n",
    "# n2_bigrams = get_top_n_gram(tw_list['text'],(2,2),20)\n",
    "# n2_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('dogecoin dogecoin dogecoin', 119),\n",
       "  ('twitter facebook instagram', 79),\n",
       "  ('deal gift gifts', 56),\n",
       "  ('facebook instagram dogecoin', 43),\n",
       "  ('instagram dogecoin socialmedia', 43),\n",
       "  ('doge deal gift', 43),\n",
       "  ('tumblr twitter facebook', 42),\n",
       "  ('dogecoin socialmedia pinterest', 42),\n",
       "  ('socialmedia pinterest doge', 42),\n",
       "  ('pinterest doge deal', 42),\n",
       "  ('linkedin twitter facebook', 36),\n",
       "  ('socialmedia pinterest dogecoin', 24),\n",
       "  ('catecoin catecoin io', 23),\n",
       "  ('gift gifts giftideas', 14),\n",
       "  ('facebook instagram tiktok', 13),\n",
       "  ('instagram tiktok socialmedia', 13),\n",
       "  ('tiktok socialmedia pinterest', 13),\n",
       "  ('pinterest dogecoin deal', 13),\n",
       "  ('dogecoin deal gift', 13),\n",
       "  ('facebook instagram bitcoin', 12)],\n",
       " <495x1933 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 2960 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n3_trigram\n",
    "n3_trigrams = get_top_n_gram(tw_list['text'],(3,3),20)\n",
    "n3_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL BUILDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>punct</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>nonstop</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...</td>\n",
       "      <td>RT VikingFlki ProTheDoge üí£VIKING FLOKI PRESALE...</td>\n",
       "      <td>[rt, vikingflki, prothedoge, viking, floki, pr...</td>\n",
       "      <td>[rt, vikingflki, prothedoge, viking, floki, pr...</td>\n",
       "      <td>[rt, vikingflki, prothedog, vike, floki, presa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Doge_BUSD: üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT, Follow us...</td>\n",
       "      <td>RT DogeBUSD üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT Follow us and...</td>\n",
       "      <td>[rt, dogebusd, giveaway, rt, follow, us, and, ...</td>\n",
       "      <td>[rt, dogebusd, giveaway, rt, follow, us, tag, ...</td>\n",
       "      <td>[rt, dogebusd, giveaway, rt, follow, us, tag, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @dream9kk: Elon Musk Thinks Dogecoin Is Sup...</td>\n",
       "      <td>RT dreamkk Elon Musk Thinks Dogecoin Is Superi...</td>\n",
       "      <td>[rt, dreamkk, elon, musk, thinks, dogecoin, is...</td>\n",
       "      <td>[rt, dreamkk, elon, musk, thinks, dogecoin, su...</td>\n",
       "      <td>[rt, dreamkk, elon, musk, think, dogecoin, sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @MilgateTyler: \"Her hair, long, black and f...</td>\n",
       "      <td>RT MilgateTyler Her hair long black and flowin...</td>\n",
       "      <td>[rt, milgatetyler, her, hair, long, black, and...</td>\n",
       "      <td>[rt, milgatetyler, hair, long, black, flowing,...</td>\n",
       "      <td>[rt, milgatetyl, hair, long, black, flow, grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...</td>\n",
       "      <td>RT VikingFlki ProTheDoge üí£VIKING FLOKI PRESALE...</td>\n",
       "      <td>[rt, vikingflki, prothedoge, viking, floki, pr...</td>\n",
       "      <td>[rt, vikingflki, prothedoge, viking, floki, pr...</td>\n",
       "      <td>[rt, vikingflki, prothedog, vike, floki, presa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...   \n",
       "1  RT @Doge_BUSD: üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT, Follow us...   \n",
       "2  RT @dream9kk: Elon Musk Thinks Dogecoin Is Sup...   \n",
       "3  RT @MilgateTyler: \"Her hair, long, black and f...   \n",
       "4  RT @VikingFl0ki: @ProTheDoge üí£VIKING FLOKI PRE...   \n",
       "\n",
       "                                               punct  \\\n",
       "0  RT VikingFlki ProTheDoge üí£VIKING FLOKI PRESALE...   \n",
       "1  RT DogeBUSD üöÄüìàüíéGiveawayüíéüìàüöÄ\\n\\nRT Follow us and...   \n",
       "2  RT dreamkk Elon Musk Thinks Dogecoin Is Superi...   \n",
       "3  RT MilgateTyler Her hair long black and flowin...   \n",
       "4  RT VikingFlki ProTheDoge üí£VIKING FLOKI PRESALE...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [rt, vikingflki, prothedoge, viking, floki, pr...   \n",
       "1  [rt, dogebusd, giveaway, rt, follow, us, and, ...   \n",
       "2  [rt, dreamkk, elon, musk, thinks, dogecoin, is...   \n",
       "3  [rt, milgatetyler, her, hair, long, black, and...   \n",
       "4  [rt, vikingflki, prothedoge, viking, floki, pr...   \n",
       "\n",
       "                                             nonstop  \\\n",
       "0  [rt, vikingflki, prothedoge, viking, floki, pr...   \n",
       "1  [rt, dogebusd, giveaway, rt, follow, us, tag, ...   \n",
       "2  [rt, dreamkk, elon, musk, thinks, dogecoin, su...   \n",
       "3  [rt, milgatetyler, hair, long, black, flowing,...   \n",
       "4  [rt, vikingflki, prothedoge, viking, floki, pr...   \n",
       "\n",
       "                                             stemmed  \n",
       "0  [rt, vikingflki, prothedog, vike, floki, presa...  \n",
       "1  [rt, dogebusd, giveaway, rt, follow, us, tag, ...  \n",
       "2  [rt, dreamkk, elon, musk, think, dogecoin, sup...  \n",
       "3  [rt, milgatetyl, hair, long, black, flow, grea...  \n",
       "4  [rt, vikingflki, prothedog, vike, floki, presa...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0‚Äì9]+', '', text)\n",
    "    return text\n",
    "df['punct'] = df['text'].apply(lambda x: remove_punct(x))\n",
    "#Appliyng tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "df['tokenized'] = df['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "df['nonstop'] = df['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "#Applying Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "df['stemmed'] = df['nonstop'].apply(lambda x: stemming(x))\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove punctuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    "    vec = TfidfVectorizer(ngram_range=ngram_range,stop_words = 'english',max_df=1.0, min_df=0.0).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n], bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.5859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.5255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.309028</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.351515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity  subjectivity  sentiment  neg    neu    pos  compound\n",
       "0  0.000000      0.000000        0.0  0.0  1.000  0.000    0.0000\n",
       "1  0.000000      0.000000        0.0  0.0  1.000  0.000    0.0000\n",
       "2  0.000000      0.350000        1.0  0.0  0.833  0.167    0.5859\n",
       "3  0.000000      0.000000        1.0  0.0  0.674  0.326    0.5255\n",
       "4  0.000000      0.000000        0.0  0.0  1.000  0.000    0.0000\n",
       "5  0.000000      0.000000        0.0  0.0  1.000  0.000    0.0000\n",
       "6  0.309028      0.729167        0.0  0.0  1.000  0.000    0.0000\n",
       "7  0.445455      0.351515        0.0  0.0  1.000  0.000    0.0000\n",
       "8 -0.400000      0.400000        0.0  0.0  1.000  0.000    0.0000\n",
       "9  0.000000      0.000000        0.0  0.0  1.000  0.000    0.0000"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\",\" \",x)\n",
    "df[\"clean_text\"] = df.text.map(remove_rt).map(rt)\n",
    "df[\"clean_text\"] = df.clean_text.str.lower()\n",
    "\n",
    "top_list,vec = get_top_n_gram(df['clean_text'], (2,3),500)\n",
    "\n",
    "#Calculating Negative, Positive, Neutral and Compound values\n",
    "df_probs = pd.DataFrame()\n",
    "df_probs[['polarity', 'subjectivity']] = df['clean_text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in df['text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        df_probs.loc[index, 'sentiment'] = 0\n",
    "    elif pos > neg:\n",
    "        df_probs.loc[index, 'sentiment'] = 1\n",
    "    else:\n",
    "        df_probs.loc[index, 'sentiment'] = 0\n",
    "    df_probs.loc[index, 'neg'] = neg\n",
    "    df_probs.loc[index, 'neu'] = neu\n",
    "    df_probs.loc[index, 'pos'] = pos\n",
    "    df_probs.loc[index, 'compound'] = comp\n",
    "df_probs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions\n",
      "0.0    524\n",
      "1.0     99\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7849117174959872"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(vec.todense(),df_probs['sentiment'],random_state=1)\n",
    "\n",
    "# kmeans = KMeans(n_clusters=3)\n",
    "nb = MultinomialNB()\n",
    "# dt = DecisionTreeClassifier()\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "pca_vec = pca.fit_transform(vec.todense())\n",
    "\n",
    "df_pc = pd.DataFrame(pca_vec,columns=['PC_1'])\n",
    "df_pc.reset_index(inplace=True,drop=True)\n",
    "df_compound = pd.DataFrame(df_probs.compound)\n",
    "df_compound.reset_index(inplace=True,drop=True)\n",
    "\n",
    "X = pd.concat([df_pc,df_compound],axis=1,ignore_index=True)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_scaled = ss.fit_transform(X)\n",
    "\n",
    "# kmeans.fit(X_scaled)\n",
    "nb.fit(X_train,y_train)\n",
    "# dt.fit(X_train,y_train)\n",
    "\n",
    "# y_preds = kmeans.predict(X_scaled)\n",
    "y_preds = nb.predict(X_test)\n",
    "# y_preds = dt.predict(X_test)\n",
    "\n",
    "# df_fin = df.copy(deep=True)\n",
    "\n",
    "df_fin = pd.DataFrame()\n",
    "\n",
    "df_fin['predictions'] = y_preds\n",
    "\n",
    "print(df_fin.value_counts('predictions'))\n",
    "\n",
    "# print(silhouette_score(vec,y_preds))\n",
    "\n",
    "accuracy_score(y_test,df_fin['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(623,)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34831460674157305"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum()/623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495, 1)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495, 2)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fa0dd1d2dc0>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,10))\n",
    "\n",
    "ax.scatter(x=X_scaled[:,0],y=X_scaled[:,1],c=y_preds,label=y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    288\n",
       "2    120\n",
       "0     87\n",
       "Name: predictions, dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Elon's bastard, damn coin. It's been down for 6 months. #ElonMusk #doge #dogecoin #dogeusdt #btc #bitcoin #bnb‚Ä¶ https://t.co/443TJh4ZiQ\""
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_fin[df_fin.predictions == 0].head(50)['predictions'][29])\n",
    "df_fin[df_fin.predictions == 0].head(50)['text'][29]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a3d059f376a9d0551670ac739dcc834dd342b8d7d90019c6bdbef463e084516"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('learn-env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
